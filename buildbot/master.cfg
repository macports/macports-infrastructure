####### -*- python -*-
# ex: set syntax=python:
# vim:ft=python

# This is a buildmaster config file. It must be installed as
# 'master.cfg' in your buildmaster's base directory.

import datetime
from itertools import ifilter, imap, izip
import json
import os
import re
import subprocess

from buildbot.plugins import buildslave, changes, schedulers, status, steps, util
from buildbot.process.properties import Property
# In 0.8.12, WebStatus can't be used as a plugin because it doesn't
# actually implement the IStatusReceiver interface, as it claims to.
from buildbot.status.web.baseweb import WebStatus
from twisted.internet import defer

####### HELPER FUNCTIONS #######

def _path(name):
    return os.path.join(os.path.dirname(__file__), name)

def merge_dicts(*dicts):
    res = {}
    for d in dicts:
        res.update(d)
    return res

def port_from_path(path, sep='/'):
    components = path.split(sep)
    try:
        if (components[0] != '_resources' and components[2] in ('Portfile', 'files')):
            return components[1]
    except IndexError:
        pass
    # Might be better to throw a custom exception here?
    return None


####### CUSTOM STEPS #######

# We can not use plain steps.Git to get the sources for additional tools from
# repos such as mpbb or macports-infrastructure, as steps.Git would set the
# got_revision property. When clicking Rebuild later, this property would be
# used for checking out a specific revision the ports tree. Therefore only
# revisions of the ports tree should be stored in this property.
class GitForTools(steps.Git):
    def updateSourceProperty(self, name, value, source=''):
        pass


# This is the dictionary that the buildmaster pays attention to. We also use
# a shorter alias to save typing.
c = BuildmasterConfig = {}

config = {
    # Production or development
    'production': False,

    # Connections
    'slaveport': 9989,
    'httpport': 8010,

    # External configuration. Use absolute paths when overriding these.
    'configfile': _path('config.json'),
    'secretsfile': _path('secrets.json'),
    'workersfile': _path('slaves.json'),
    'htpasswdfile': _path('htpasswd'),

    # GitHub. Repository URLs must have the ".git" suffix.
    'githubsecretfile': _path('github.secret'),
    'baseurl': 'https://github.com/macports/macports-base.git',
    'mpbburl': 'https://github.com/macports/mpbb.git',
    'portsurl': 'https://github.com/macports/macports-ports.git',
    'wwwurl': 'https://github.com/macports/macports-www.git',
    'guideurl': 'https://github.com/macports/macports-guide.git',
    'infraurl': 'https://github.com/macports/macports-infrastructure.git',

    # Tooling
    'slaveprefix': '/opt/local',
    'toolsprefix': '/opt/mports',
    'jobstoolsprefix': '/opt/local',

    # Deployment
    'archivesite': 'http://packages.macports.org',
    'archivesiteprivate': 'https://packages-private.macports.org',
    'privkey': '',
    'deploy': {},

    # Site definitions
    # (http://docs.buildbot.net/0.8.12/manual/cfg-global.html#site-definitions)
    'title': 'MacPorts',
    'titleurl': 'https://www.macports.org/',
    'buildboturl': 'http://localhost:8010/',

    # Database
    # (http://docs.buildbot.net/0.8.12/manual/cfg-global.html#database-specification)
    'db_url': 'sqlite:///state.sqlite',

    # Data lifetime
    # (http://docs.buildbot.net/0.8.12/manual/cfg-global.html#data-lifetime)
    'buildhorizon': 10000,
    'cachedbuildrequests': 20000,
    'cachedbuilds': 600,
    'cachedchanges': 200,
    'eventhorizon': 2000,
    'loghorizon': 5000
    }

# Override defaults with external settings.
try:
    with open(config['configfile']) as f:
        extconfig = json.load(f)
except IOError:
    extconfig = {}
config.update(extconfig)

# Load secrets from external file
try:
    with open(config['secretsfile']) as f:
        secrets = json.load(f)
except IOError:
    secrets = {}

path_base = '/usr/bin:/bin:/usr/sbin:/sbin'
path_ports = os.path.join(config['toolsprefix'], 'bin') + ':' + path_base
path_jobs = os.path.join(config['jobstoolsprefix'], 'bin') + ':' + path_base

# Allow spaces and tabs in property values
c['validation'] = {'property_value': re.compile(r'^[ \t\w./~:-]*$')}


####### BUILDSLAVES #######

# The 'slaves' list defines the set of recognized buildslaves. Each element is
# a BuildSlave object, specifying a unique slave name and password.  The same
# slave name and password must be configured on the slave.

with open(config['workersfile']) as f:
    slavedata = json.load(f)

# convert unicode to byte strings
build_platforms = [s.encode('utf-8') for s in slavedata['build_platforms']]

c['slaves'] = [buildslave.BuildSlave(name, pwd)
               for name, pwd in slavedata['slaves'].iteritems()]

# 'slavePortnum' defines the TCP port to listen on for connections from slaves.
# This must match the value configured into the buildslaves (with their
# --master option).
c['slavePortnum'] = config['slaveport']


####### WEB STATUS AND CHANGE HOOKS #######

c['status'] = []

# WebStatus [1] runs a web server that serves the web interface. It can
# also accept HTTP requests at a hook endpoint [2] and translate them
# into Changes. We're using the GitHub hook [3] to accept webhook
# payloads [4].
#
# [1]: http://docs.buildbot.net/0.8.12/manual/cfg-statustargets.html#webstatus
# [2]: http://docs.buildbot.net/0.8.12/manual/cfg-statustargets.html#change-hooks
# [3]: http://docs.buildbot.net/0.8.12/manual/cfg-statustargets.html#github-hook
# [4]: https://developer.github.com/webhooks

if config['production']:
    with open(config['githubsecretfile']) as f:
        githubsecret = f.readline().rstrip('\n')
    change_hook_kwargs = {
        'change_hook_dialects': {
            'github': {
                'secret': githubsecret,
                'strict': True
                }
            }
        }
else:
    # TODO Add alternate change source, probably a GitPoller.
    change_hook_kwargs = {}

c['status'].append(
    WebStatus(
        http_port=config['httpport'],
        authz=util.Authz(
            auth=util.HTPasswdAprAuth(config['htpasswdfile']),
            gracefulShutdown='auth',
            forceBuild='auth',
            forceAllBuilds='auth',
            pingBuilder='auth',
            stopBuild='auth',
            stopAllBuilds='auth',
            cancelPendingBuild='auth'),
        **change_hook_kwargs))

# GitHub status updates
# https://docs.buildbot.net/0.8.14/manual/cfg-statustargets.html#githubstatus

from buildbot.process import properties

class GitHubStatusWithFilter(status.GitHubStatus):

    def __init__(self, buildername_filter_fn, *args, **kwargs):
        self._buildername_filter_fn = buildername_filter_fn
        super(GitHubStatusWithFilter, self).__init__(*args, **kwargs)

    def buildStarted(self, builderName, build):
        if self._buildername_filter_fn(builderName):
            super(GitHubStatusWithFilter, self).buildStarted(builderName, build)

    def buildFinished(self, builderName, build, results):
        if self._buildername_filter_fn(builderName):
            super(GitHubStatusWithFilter, self).buildFinished(builderName, build, results)

@properties.renderer
def getGitHubRepoOwner(props):
    # project is in format "owner/name"
    project = props.getProperty('project')
    owner = project.split('/')[0]
    return owner

@properties.renderer
def getGitHubRepoName(props):
    # project is in format "owner/name"
    project = props.getProperty('project')
    name = project.split('/')[1]
    return name

buildername_regex = re.compile(r'^(base-*|ports-.*-watcher|jobs-guide|jobs-man|jobs-www)$')
def buildername_filter(buildername):
    return bool(buildername_regex.match(buildername))

if 'githubapitoken' in secrets:
    # This GitHub API token must at least have the repo:status scope
    # TODO: buildbot 0.8.14 also supports setting context=util.Interpolate("buildbot/%(prop:buildername)s")
    c['status'].append(
        GitHubStatusWithFilter(
            buildername_filter_fn=buildername_filter,
            token=secrets['githubapitoken'],
            repoOwner=getGitHubRepoOwner,
            repoName=getGitHubRepoName,
            sha=util.Interpolate("%(src::revision)s"),
            startDescription='Build started.',
            endDescription='Build done.'))

####### SCHEDULERS #######

base_platforms = [plat for plat in build_platforms if 'legacy' not in plat and '10.6_i386' not in plat]
port_platforms = [plat for plat in build_platforms if 'linux' not in plat and '10.5_ppc' != plat]

base_buildernames = map('base-{}'.format, base_platforms)
portwatcher_buildernames = map('ports-{}-watcher'.format, port_platforms)
portbuilder_buildernames = map('ports-{}-builder'.format, port_platforms)
portbuilder_triggerables = map('ports-{}-trigger'.format, port_platforms)

# The ChangeFilters assume that Git URLs end with ".git".
c['schedulers'] = [
    schedulers.SingleBranchScheduler(
        name='base',
        treeStableTimer=5,
        change_filter=util.ChangeFilter(
            repository=config['baseurl'][:-4],
            branch_re='^(master|release-.*)$'),
        builderNames=base_buildernames),
    schedulers.SingleBranchScheduler(
        name='ports',
        # Don't start a separate build for every pushed commit.
        treeStableTimer=5,
        change_filter=util.ChangeFilter(
            repository=config['portsurl'][:-4],
            branch='master',
            # Should actually skip changes to files/ only, but only if
            # we know the last build of the port succeeded.
            filter_fn=lambda change: any(port_from_path(f) for f in change.files)),
        builderNames=portwatcher_buildernames),
    schedulers.ForceScheduler(
        name='base_force',
        builderNames=base_buildernames),
#    schedulers.ForceScheduler(
#        name='portbuilder_force',
#        builderNames=portbuilder_buildernames,
#        properties=[util.StringParameter(
#            name='portname',
#            label='Port name:',
#            default='',
#            required=True)
#        ]),
    schedulers.ForceScheduler(
        name='portwatcher_force',
        builderNames=portwatcher_buildernames,
        properties=[util.StringParameter(
            name='portlist',
            label='Port list:',
            default='',
            size=30,
            required=True)])
    ]


if 'www' in config['deploy']:
    c['schedulers'].extend((
        schedulers.SingleBranchScheduler(
            name='www',
            treeStableTimer=300,
            change_filter=util.ChangeFilter(
                repository=config['wwwurl'][:-4],
                branch='master'),
            builderNames=['jobs-www']),
        schedulers.ForceScheduler(
            name='www_force',
            builderNames=['jobs-www'])
        ))

if 'guide' in config['deploy']:
    c['schedulers'].extend((
        schedulers.SingleBranchScheduler(
            name='guide',
            treeStableTimer=300,
            change_filter=util.ChangeFilter(
                repository=config['guideurl'][:-4],
                branch='master'),
            builderNames=['jobs-guide']),
        schedulers.ForceScheduler(
            name='guide_force',
            builderNames=['jobs-guide'])
        ))

if 'man' in config['deploy']:
    c['schedulers'].extend((
        schedulers.SingleBranchScheduler(
            name='man',
            treeStableTimer=300,
            change_filter=util.ChangeFilter(
                repository=config['baseurl'][:-4],
                # FIXME: determine the current stable version or use
                # a special branch name instead of hardcoding a version
                branch='release-2.5'),
            builderNames=['jobs-man']),
        schedulers.ForceScheduler(
            name='man_force',
            builderNames=['jobs-man'])
        ))

if 'portindex' in config['deploy']:
    c['schedulers'].extend((
        schedulers.SingleBranchScheduler(
            name='portindex',
            treeStableTimer=300,
            change_filter=util.ChangeFilter(
                repository=config['portsurl'][:-4],
                branch='master'),
            builderNames=['jobs-portindex']),
        schedulers.ForceScheduler(
            name='portindex_force',
            builderNames=['jobs-portindex'])
        ))

if 'mirror' in config['deploy']:
    c['schedulers'].append(
        schedulers.Triggerable(
            name='mirror',
            builderNames=['jobs-mirror'])
        )

portbuilders = izip(portbuilder_triggerables, portbuilder_buildernames)
c['schedulers'].extend(schedulers.Triggerable(name=t, builderNames=[b])
                       for t, b in portbuilders)


####### BUILDERS #######

# WARNING: mergeRequests has to be False or Triggerable builds will not be scheduled correctly!
c['mergeRequests'] = False

# The 'builders' list defines the Builders, which tell Buildbot how to perform a build:
# what steps, and which slaves can execute them.  Note that any particular build will
# only take place on one slave.

base_factory = util.BuildFactory()
base_factory.workdir = '../build'

# Set progress=True on Git steps to prevent timeouts on slow fetches.
base_factory.addStep(steps.Git(
    repourl=config['baseurl'],
    progress=True,
    mode='full',
    method='copy',
    env={'PATH': path_ports}))
base_factory.addStep(steps.Configure(command=util.WithProperties("""
env PATH=/usr/bin:/bin:/usr/sbin:/sbin ./configure --enable-readline \
    --prefix=%(workdir)s/opt/local \
    --with-applications-dir=%(workdir)s/opt/local/Applications \
    --with-install-user=`id -un` \
    --with-install-group=`id -gn` \
"""),logfiles={'config.log': 'config.log'}))
base_factory.addStep(steps.Compile(
    name='make -C vendor',
    command='make -j`sysctl -n hw.activecpu` -C vendor'))
base_factory.addStep(steps.Compile(
    name='make',
    command='make -j`sysctl -n hw.activecpu`'))
base_factory.addStep(steps.ShellCommand(
    command='make install',
    name='install',
    description=['installing'],
    descriptionDone=['install']))
base_factory.addStep(steps.ShellCommand(
    command='make test',
    name='test',
    description=['testing'],
    descriptionDone=['test']))
base_factory.addStep(steps.ShellCommand(
    command=util.WithProperties('make distclean; rm -rf %(workdir)s/opt/local'),
    name='clean',
    description=['cleaning'],
    descriptionDone=['clean']))

# custom class to make the file list available on the slave...
class SetPropertyFromCommandWithPortlist(steps.SetPropertyFromCommand):
    def setBuild(self, build):
        super(SetPropertyFromCommandWithPortlist, self).setBuild(build)

        # support forced build properties
        ports = set(self.getProperty('portlist', default='').split())

        # paths should be category/portdir(/...)
        ports.update(ifilter(None, imap(port_from_path, self.build.allFiles())))

        self.setProperty('fullportlist', ' '.join(ports))

    def getText(self, cmd, results):
        if self.hasProperty('subportlist'):
            return ['Port list: {}'.format(self.getProperty('subportlist'))]
        # let ShellCommand describe
        return steps.ShellCommand.getText(self, cmd, results)

# can't run with prefix inside the workdir in production,
# because archives must be built with prefix=/opt/local
if config['production']:
    prefix = '/opt/local'
    dlhost = 'packages@packages-origin.macports.org'
    dlhost_private = dlhost
    dlpath = '/var/www/html/packages'
    dlpath_private = '/var/www/html/packages-private'
else:
    prefix = config['slaveprefix']
    dlhost = ''
    dlhost_private = dlhost
    dlpath = './deployed_archives'
    dlpath_private = './deployed_archives_private'

ulpath = 'archive_staging'
ulpath_unique = ulpath+'-%(buildername)s'

@util.renderer
def make_build_url(props):
    buildername = props.getProperty('buildername')
    buildnumber = props.getProperty('buildnumber')
    url = c['buildbotURL']
    if not url.endswith('/'):
        url += '/'
    url += 'builders/%s/builds/%s' % (buildername, buildnumber)
    return url

class TriggerWithPortlist(steps.Trigger):
    def getSchedulersAndProperties(self):
        sp = []
        priority = 1
        for scheduler in self.schedulerNames:
            for port in self.build.getProperty('subportlist').split():
                props = self.set_properties.copy()
                props['portname'] = port
                props['priority'] = priority
                priority += 1
                sp.append([scheduler, props])
        return sp


# -- Port Watcher --

def make_portwatcher_factory(triggerable):
    portwatcher_factory = util.BuildFactory()
    portwatcher_factory.useProgress = False
    portwatcher_factory.workdir = '../build'

    # get mpbb; we'll do the checkout of base and dports via these scripts
    portwatcher_factory.addStep(GitForTools(
        repourl=config['mpbburl'],
        progress=True,
        env={'PATH': path_ports},
        alwaysUseLatest=True,
        workdir=os.path.join(portwatcher_factory.workdir, 'mpbb'),
        haltOnFailure=True))

    portwatcher_factory.addStep(steps.ShellCommand(
        command=['./mpbb/mpbb', '--prefix', util.WithProperties(prefix), 'cleanup'],
        name='cleanup',
        description=['cleaning'],
        descriptionDone=['clean']))

    portwatcher_factory.addStep(steps.ShellCommand(
        command=['./mpbb/mpbb', '--prefix', util.WithProperties(prefix), 'selfupdate'],
        name='selfupdate',
        description=['updating', 'MacPorts'],
        descriptionDone=['update', 'MacPorts'],
        haltOnFailure=True))

    portwatcher_factory.addStep(steps.ShellCommand(
        command=['./mpbb/mpbb', '--prefix', util.WithProperties(prefix), 'checkout',
                 '--archive-sites', config['archivesite']+' '+config['archivesiteprivate'],
                 '--ports-url', config['portsurl']],
        timeout=3600,
        name='checkout',
        description=['syncing', 'ports'],
        descriptionDone=['sync', 'ports'],
        haltOnFailure=True))

    def extract_subportlist(rc, stdout, stderr):
        """
        Extract function for SetPropertyFromCommand(). Buildbot did not get the
        capability to ignore or distinguish stderr output before 0.9.x, but
        extract_fn always had the option to deal with them separately, so do
        that.

        This is called by SetPropertyFromCommand with the return value of the
        command and strings containing stdout and stderr. The return value
        should be a dictionary of new properties to be set.
        """
        if rc != 0:
            # Set an empty subport list on error
            return {'subportlist': ''}
        subports = [x.strip() for x in stdout.splitlines()]
        return {'subportlist': ' '.join(subports)}

    portwatcher_factory.addStep(SetPropertyFromCommandWithPortlist(
        command=['./mpbb/mpbb', '--prefix', util.WithProperties(prefix), 'list-subports',
                 '--archive-site', config['archivesite'],
                 '--archive-site-private', config['archivesiteprivate'],
                 util.WithProperties('%(fullportlist)s')],
        extract_fn=extract_subportlist,
        name='subports',
        description=['listing', 'subports'],
        haltOnFailure=True))

    def has_subportlist(step):
        return step.hasProperty('subportlist') and step.getProperty('subportlist')

    if 'mirror' in config['deploy']:
        portwatcher_factory.addStep(steps.Trigger(
            name='mirror',
            schedulerNames=['mirror'],
            set_properties={'subportlist': Property('subportlist'), 'triggered_by': make_build_url},
            waitForFinish=True,
            updateSourceStamp=True,
            doStepIf=has_subportlist))

    portwatcher_factory.addStep(TriggerWithPortlist(
        name='portbuilders',
        schedulerNames=[triggerable],
        set_properties={'triggered_by': make_build_url},
        waitForFinish=True,
        updateSourceStamp=True,
        doStepIf=has_subportlist))

    # make a logfile summarising the success/failure status for each port
    # (Current approach is not so useful as it is not incremental;
    #  ideally this would already be displayed during the Trigger step.)
    portwatcher_factory.addStep(steps.ShellCommand(
        command=['cat', os.path.join(logdir, 'ports-progress.txt')],
        name='summary',
        description=['summary']))

    return portwatcher_factory

# -- Port Builder --

portbuilder_factory = util.BuildFactory()
portbuilder_factory.useProgress = False
portbuilder_factory.workdir = '../build'
logdir = os.path.join(portbuilder_factory.workdir, 'logs')

portbuilder_factory.addStep(steps.Compile(
    command=['./mpbb/mpbb', '--prefix', util.WithProperties(prefix), 'install-dependencies', util.WithProperties('%(portname)s')],
    timeout=3600,
    name='install-dependencies',
    description=['installing', 'dependencies', 'of', util.WithProperties('%(portname)s')],
    descriptionDone=['install', 'dependencies', 'of', util.WithProperties('%(portname)s')],
    logfiles={'dependencies': os.path.join(logdir, 'dependencies-progress.txt')},
    haltOnFailure=True))

portbuilder_factory.addStep(steps.Compile(
    command=['./mpbb/mpbb', '--prefix', util.WithProperties(prefix), 'install-port', util.WithProperties('%(portname)s')],
    timeout=3600,
    name='install-port',
    description=['installing', util.WithProperties('%(portname)s')],
    descriptionDone=['install', util.WithProperties('%(portname)s')],
    logfiles={'files': os.path.join(logdir, 'port-contents.txt'),
              'statistics': os.path.join(logdir, 'port-statistics.txt'),
              'main.log': os.path.join(logdir, 'main.log')},
    haltOnFailure=True))

portbuilder_factory.addStep(steps.ShellCommand(
    command=['./mpbb/mpbb', '--prefix', util.WithProperties(prefix), 'gather-archives',
             '--archive-site', config['archivesite'],
             '--archive-site-private', config['archivesiteprivate'],
             '--staging-dir', ulpath],
    name='gather-archives',
    description=['gathering', 'archives'],
    descriptionDone=['gather', 'archives'],
    haltOnFailure=True))

# upload archives from build slave to master
portbuilder_factory.addStep(steps.DirectoryUpload(
    slavesrc=ulpath,
    masterdest=util.WithProperties(ulpath_unique)))

# XXX: move deploy_archives.sh functionality to mpbb
# sign generated binaries and sync to download server (if distributable)
if config['production']:
    portbuilder_factory.addStep(steps.MasterShellCommand(
        command=['./deploy_archives.sh', util.WithProperties(os.path.join(ulpath_unique, 'public'))],
        name='deploy-archives',
        description=['deploying', 'public', 'archives'],
        descriptionDone=['deploy', 'public', 'archives'],
        env={'PRIVKEY': config['privkey'], 'DLHOST': dlhost, 'DLPATH': dlpath}))
    portbuilder_factory.addStep(steps.MasterShellCommand(
        command=['./deploy_archives.sh', util.WithProperties(os.path.join(ulpath_unique, 'private'))],
        name='deploy-archives-private',
        description=['deploying', 'private', 'archives'],
        descriptionDone=['deploy', 'private', 'archives'],
        env={'PRIVKEY': config['privkey'], 'DLHOST': dlhost_private, 'DLPATH': dlpath_private}))

# TODO: do we want to upload the individual logs so maintainers can review them?
portbuilder_factory.addStep(steps.ShellCommand(
    command=['./mpbb/mpbb', '--prefix', util.WithProperties(prefix), 'cleanup'],
    name='cleanup',
    description=['cleaning'],
    descriptionDone=['clean'],
    alwaysRun=True))

class RsyncDeployStep(steps.ShellCommand):

    def __init__(self, host, user, srcpath, destpath, **kwargs):
        super(RsyncDeployStep, self).__init__(
                name='rsync',
                description=['deploying'],
                descriptionDone=['deploy'],
                command='rsync -avzhC --delay-updates --delete-delay %s %s@%s:%s/' % (srcpath, user, host, destpath),
                env={'RSYNC_RSH': 'ssh -i ssh_key -oUserKnownHostsFile=ssh_known_hosts'},
                **kwargs)

class PostgresDeployStep(steps.ShellCommand):

    def __init__(self, host, user, sqlfile, **kwargs):
        super(PostgresDeployStep, self).__init__(
                name='psql',
                description=['deploying'],
                descriptionDone=['deploy'],
                command='ssh -C -i ssh_key -oUserKnownHostsFile=ssh_known_hosts %s@%s psql < %s' % (user, host, sqlfile),
                **kwargs)


def make_ssh_deploy_step(sshkeyfile, sshknownhostsfile, step):
    return (
        steps.FileDownload(
            name='ssh key',
            description=['transferring'],
            descriptionDone=['transfer'],
            mastersrc=sshkeyfile,
            slavedest='ssh_key',
            mode=0600),
        steps.FileDownload(
            name='ssh known_hosts',
            description=['transferring'],
            descriptionDone=['transfer'],
            mastersrc=sshknownhostsfile,
            slavedest='ssh_known_hosts',
            mode=0600),
        step)


class CDNPurgeStep(steps.MasterShellCommand):
    # XXX: Ensure py*-requests-oauthlib is available

    def __init__(self, zoneid, **kwargs):
        super(CDNPurgeStep, self).__init__(
                name='cdn',
                description=['purging', 'CDN zone'],
                descriptionDone=['purge', 'CDN zone'],
                command="./maxcdn-purge/maxcdn-purge.py %s %s" % (zoneid, config['deploy']['maxcdn']['secrets']),
                env={'PATH': "${PATH}"},
                **kwargs)


if 'www' in config['deploy']:
    jobs_www_factory = util.BuildFactory()
    # TODO: incremental mode with cleanup?
    jobs_www_factory.addStep(steps.Git(
        repourl=config['wwwurl'],
        progress=True,
        mode='full',
        method='copy',
        workdir='build/www',
        haltOnFailure=True))
    jobs_www_factory.addStep(steps.Compile(
        name='lint',
        description=['linting'],
        descriptionDone=['lint'],
        command='make lint',
        workdir='build/www'))
    jobs_www_factory.addSteps(
        make_ssh_deploy_step(
            sshkeyfile=config['deploy']['www']['sshkeyfile'],
            sshknownhostsfile=config['deploy']['www']['sshknownhostsfile'],
            step=RsyncDeployStep(
                host=config['deploy']['www']['host'],
                user=config['deploy']['www']['user'],
                srcpath='www/',
                destpath=config['deploy']['www']['destpath'])))
    if 'maxcdn' in config['deploy'] and 'maxcdnzoneid' in config['deploy']['www']:
        jobs_www_factory.addStep(CDNPurgeStep(zoneid=config['deploy']['www']['maxcdnzoneid']))

if 'portindex' in config['deploy']:
    jobs_portindex_factory = util.BuildFactory()
    # TODO: incremental mode with cleanup?
    jobs_portindex_factory.addStep(GitForTools(
        name='git infrastructure',
        repourl=config['infraurl'],
        progress=True,
        alwaysUseLatest=True,
        mode='full',
        method='copy',
        workdir='build/infrastructure',
        haltOnFailure=True))
    jobs_portindex_factory.addStep(steps.Git(
        name='git ports',
        repourl=config['portsurl'],
        progress=True,
        alwaysUseLatest=True,
        mode='incremental',
        workdir='build/ports',
        haltOnFailure=True))
    jobs_portindex_factory.addStep(steps.ShellCommand(
        command='portindex',
        name='portindex',
        description=['indexing', 'ports'],
        descriptionDone=['index', 'ports'],
        workdir='build/ports',
        haltOnFailure=True))
    jobs_portindex_factory.addStep(steps.ShellCommand(
        command='port-tclsh infrastructure/jobs/portindex2postgres.tcl',
        name='portindex2postgres',
        description=['converting', 'index to SQL'],
        descriptionDone=['convert', 'index to SQL'],
        haltOnFailure=True))
    jobs_portindex_factory.addSteps(
        make_ssh_deploy_step(
            sshkeyfile=config['deploy']['portindex']['sshkeyfile'],
            sshknownhostsfile=config['deploy']['portindex']['sshknownhostsfile'],
            step=PostgresDeployStep(
                host=config['deploy']['portindex']['host'],
                user=config['deploy']['portindex']['user'],
                sqlfile='PortIndex.sql')))

if 'mirror' in config['deploy']:
    jobs_mirror_factory = util.BuildFactory()
    # This is the wrong absolute path! It's relative to the master; we want it relative to the worker.
    jobs_mirror_prefix = os.path.abspath(os.path.join(jobs_mirror_factory.workdir, 'prefix'))

    # get mpbb; we'll do the checkout of base and dports via these scripts
    jobs_mirror_factory.addStep(GitForTools(
        repourl=config['mpbburl'],
        progress=True,
        env={'PATH': path_jobs},
        alwaysUseLatest=True,
        workdir=os.path.join(jobs_mirror_factory.workdir, 'mpbb'),
        haltOnFailure=True))

    jobs_mirror_factory.addStep(steps.ShellCommand(
        command=['./mpbb/mpbb', '--prefix', jobs_mirror_prefix, 'selfupdate'],
        name='selfupdate',
        description=['updating', 'MacPorts'],
        descriptionDone=['update', 'MacPorts'],
        haltOnFailure=True))

    jobs_mirror_factory.addStep(steps.ShellCommand(
        command=['./mpbb/mpbb', '--prefix', jobs_mirror_prefix, 'checkout',
                 '--ports-url', config['portsurl']],
        timeout=3600,
        name='checkout',
        description=['syncing', 'ports'],
        descriptionDone=['sync', 'ports'],
        haltOnFailure=True))

    @util.renderer
    def make_jobs_mirror_command(props):
        cmd = ['./mpbb/mpbb', '--prefix', jobs_mirror_prefix, 'mirror-distfiles', '--distfiles-dir', config['deploy']['mirror']['distfilesdir']]
        if props.hasProperty('subportlist') and props.getProperty('subportlist') is not None:
            cmd += props.getProperty('subportlist').split()
        return cmd

    jobs_mirror_factory.addStep(steps.ShellCommand(
        command=make_jobs_mirror_command,
        name='mirror',
        description=['mirroring', 'distfiles'],
        descriptionDone=['mirror', 'distfiles']))

if 'guide' in config['deploy']:
    jobs_guide_factory = util.BuildFactory()
    # TODO: incremental mode with cleanup?
    jobs_guide_factory.addStep(steps.Git(
        repourl=config['guideurl'],
        progress=True,
        mode='full',
        method='copy',
        workdir='build/guide',
        haltOnFailure=True))
    # TODO: check for existence of tools in toolsprefix
    jobs_guide_factory.addStep(steps.Compile(
        name='validate',
        description=['validating'],
        descriptionDone=['validate'],
        command='make validate',
        workdir='build/guide',
        env={'PATH': path_jobs},
        haltOnFailure=True))
    jobs_guide_factory.addStep(steps.Compile(
        command='make all',
        workdir='build/guide',
        env={'PATH': path_jobs},
        haltOnFailure=True))
    jobs_guide_factory.addSteps(
        make_ssh_deploy_step(
            sshkeyfile=config['deploy']['guide']['sshkeyfile'],
            sshknownhostsfile=config['deploy']['guide']['sshknownhostsfile'],
            step=RsyncDeployStep(
                host=config['deploy']['guide']['host'],
                user=config['deploy']['guide']['user'],
                srcpath='guide/guide/html/',
                destpath=config['deploy']['guide']['destpath'])))
    if 'maxcdn' in config['deploy'] and 'maxcdnzoneid' in config['deploy']['guide']:
        jobs_guide_factory.addStep(CDNPurgeStep(zoneid=config['deploy']['guide']['maxcdnzoneid']))

if 'man' in config['deploy']:
    jobs_man_factory = util.BuildFactory()
    # TODO: incremental mode with cleanup?
    jobs_man_factory.addStep(steps.Git(
        repourl=config['baseurl'],
        progress=True,
        mode='full',
        method='copy',
        haltOnFailure=True))
    jobs_man_factory.addStep(steps.Configure(
        command="./standard_configure.sh",
        logfiles={'config.log': 'config.log'},
        haltOnFailure=True))
    jobs_man_factory.addStep(steps.Compile(
        command='make -C vendor/tcl/unix tclsh',
        name='make tclsh',
        description=['compiling', 'tclsh'],
        descriptionDone=['compile', 'tclsh'],
        warnOnWarnings=False,
        haltOnFailure=True))
    # TODO: check for existence of tools in toolsprefix
    jobs_man_factory.addStep(steps.ShellCommand(
        command='make -C doc html prefix=' + config['jobstoolsprefix'],
        name='make',
        description=['making', 'man pages'],
        descriptionDone=['make', 'man pages'],
        env={'PATH': path_jobs},
        haltOnFailure=True))
    jobs_man_factory.addSteps(
        make_ssh_deploy_step(
            sshkeyfile=config['deploy']['man']['sshkeyfile'],
            sshknownhostsfile=config['deploy']['man']['sshknownhostsfile'],
            step=RsyncDeployStep(
                host=config['deploy']['man']['host'],
                user=config['deploy']['man']['user'],
                srcpath='doc/*.html',
                destpath=config['deploy']['man']['destpath'])))


####### BUILDER CONFIGURATION #######

def getPriority(request):
    if request.properties and request.properties.hasProperty('priority'):
        return int(request.properties.getProperty('priority'))
    else:
        return math.inf

def getNextBuildOnPortBuilder(builder, requests):
    nextBuild = requests[0]
    for request in requests:
        if getPriority(request) < getPriority(nextBuild):
            nextBuild = request
    return nextBuild

# XXX: slavenames assignment should be automatic and more generic
portsslaves = {}
baseslaves = {}
slavenames = slavedata['slaves'].keys()
for plat in build_platforms:
    baseslaves[plat]  = filter(lambda x: x.endswith(plat+'-base'),  slavenames)
    portsslaves[plat] = filter(lambda x: x.endswith(plat+'-ports'), slavenames)

env_buildinfo = {
    'BUILDBOT_BUILDERNAME': util.WithProperties('%(buildername)s'),
    'BUILDBOT_BUILDNUMBER': util.WithProperties('%(buildnumber)s'),
    'BUILDBOT_BUILDURL': make_build_url
    }

c['builders'] = []
extract_os = re.compile(r'10\.\d+')
for plat in build_platforms:
    os_match = extract_os.search(plat)
    os_version = os_match.group(0) if os_match else plat
    if 'legacy' not in plat and '10.6_i386' not in plat:
        c['builders'].append(
            util.BuilderConfig(
                name='base-' + plat,
                slavenames=['base-' + plat],
                factory=base_factory,
                tags=['base', os_version],
                env=merge_dicts(env_buildinfo, {'PATH': path_base})))
    if 'linux' not in plat and '10.5_ppc' != plat:
        c['builders'].extend((
            util.BuilderConfig(
                name='ports-' + plat + '-watcher',
                slavenames=['ports-' + plat],
                factory=make_portwatcher_factory('ports-' + plat + '-trigger'),
                tags=['portwatcher', os_version],
                env=merge_dicts(env_buildinfo, {'PATH': path_ports})),
            util.BuilderConfig(
                name='ports-' + plat + '-builder',
                slavenames=['ports-' + plat],
                factory=portbuilder_factory,
                tags=['portbuilder', os_version],
                nextBuild=getNextBuildOnPortBuilder,
                env=merge_dicts(env_buildinfo, {'PATH': path_ports}))
            ))

if 'www' in config['deploy']:
    c['builders'].append(
        util.BuilderConfig(
            name='jobs-www',
            slavenames=['jobs'],
            factory=jobs_www_factory,
            tags=['jobs', 'docs', 'www'],
            env=merge_dicts(env_buildinfo, {'PATH': path_jobs})))
if 'portindex' in config['deploy']:
    c['builders'].append(
        util.BuilderConfig(
            name='jobs-portindex',
            slavenames=['jobs'],
            factory=jobs_portindex_factory,
            tags=['jobs', 'portindex', 'www'],
            env=merge_dicts(env_buildinfo, {'PATH': path_jobs})))
if 'guide' in config['deploy']:
    c['builders'].append(
        util.BuilderConfig(
            name='jobs-guide',
            slavenames=['jobs'],
            factory=jobs_guide_factory,
            tags=['jobs', 'docs', 'guide'],
            env=merge_dicts(env_buildinfo, {'PATH': path_jobs})))
if 'man' in config['deploy']:
    c['builders'].append(
        util.BuilderConfig(
            name='jobs-man',
            slavenames=['jobs'],
            factory=jobs_man_factory,
            tags=['jobs', 'docs', 'man'],
            env=merge_dicts(env_buildinfo, {'PATH': path_jobs})))
if 'mirror' in config['deploy']:
    c['builders'].append(
        util.BuilderConfig(
            name='jobs-mirror',
            slavenames=['jobs'],
            factory=jobs_mirror_factory,
            tags=['jobs', 'mirror', 'distfiles'],
            env=merge_dicts(env_buildinfo, {'PATH': path_jobs})))


####### MAIL NOTIFIERS #######

# TODO: This is the old mail notifier;
# - useful functionality could be copied
# - then the code should be removed
#
# notifier that sends mail to last committers and maintainers of failed ports
class OldPortsMailNotifier(status.MailNotifier):
    # would make more sense to override getInterestedUsers() in BuildStatus,
    # but it seems almost impossible to tell a builder to use a different
    # class for status in its Build objects
    def useLookup(self, build):
        failedPorts = set()
        interestedUsers = set()

        # XXX: needs to be rewritten for the new steps of mpbb
        statusStep = [x for x in build.getSteps() if x.getName() == 'status'][0]
        statusLog = [x for x in statusStep.getLogs() if x.getName() == 'portstatus'][0]
        for line in statusLog.getText().splitlines():
            halves = line.split()
            if halves[0] == '[FAIL]':
                failedPorts.add(halves[1])
        
        fakeAddresses = {'nomaintainer', 'nomaintainer@macports.org', 'openmaintainer', 'openmaintainer@macports.org'}
        for p in failedPorts:
            output = subprocess.Popen(['/opt/local/bin/port', 'info', '--index', '--maintainers', '--line', p], stdout=subprocess.PIPE).communicate()[0].strip()
            for m in output.split(','):
                if m not in fakeAddresses:
                    interestedUsers.add(m)

        ss = build.getSourceStamp()
        if ss:
            for c in ss.changes:
                interesting = False
                for f in c.files:
                    comps = f.split('/')
                    if len(comps) >= 3 and comps[2] in failedPorts and comps[0] == 'dports' and comps[1] != '_resources':
                        interesting = True
                        break
                if interesting:
                    interestedUsers.add(c.who)

        dl = []
        for u in interestedUsers:
            d = defer.maybeDeferred(self.lookup.getAddress, u)
            dl.append(d)
        return defer.gatherResults(dl)

class PortsMailNotifier(status.MailNotifier, object):
    def __init__(self, fromaddr, *args, **kwargs):
        self.interested_users = set()
        self.portMessageFormatter = kwargs.pop('portMessageFormatter')
        super(PortsMailNotifier, self).__init__(fromaddr=fromaddr, *args, **kwargs)

    # same as original, but calls portMessageFormatter with access to interested_users
    def buildMessageDict(self, name, build, results):
        msgdict, self.interested_users = self.portMessageFormatter(
            self.mode, name, build, results, self.master_status)
        return msgdict

    def useLookup(self, build):
        # Initialize with additional recipients.
        dl = [defer.maybeDeferred(self.lookup.getAddress, user)
              for user in self.interested_users]

        # original list of recipients
#        for u in build.getResponsibleUsers() + build.getInterestedUsers():
#            d = defer.maybeDeferred(self.lookup.getAddress, u)
#            dl.append(d)
        return defer.gatherResults(dl)

portWatcherMessageFormatter_pattern = re.compile(r"^Building '(?P<port>.*?)'.*?(\(failed to install dependency '(?P<dependency>.*?)'\))?( maintainers: (?P<maintainers>.*?)\.)?$")

def portWatcherMessageFormatter(mode, name, build, results, master_status):
    interested_users = set()
    result = util.Results[results]
    subject = 'Build {} on {}'.format(result.title(), build.getSlavename())
    text = list()
    text.append('Status:       {}'.format(result.title()))
    text.append('Build slave:  {}'.format(build.getSlavename()))
    build_url = master_status.getURLForThing(build)
    if build_url:
        text.append('Full logs:    {}'.format(build_url))
        text.append('Build reason: {}'.format(build.getReason()))
        text.append('Port list:    {}'.format(build.getProperty('fullportlist')))
        text.append('Subport list:\n\t- {}'.format(build.getProperty('subportlist').replace(' ', '\n\t- ')))
        text.append('Variants:     {}'.format(build.getProperty('variants')))
        text.append('Revision:     {}'.format(build.getProperty('revision')))
        text.append('Build time:   {}'.format(datetime.timedelta(seconds=int(round(build.getTimes()[1] - build.getTimes()[0])))))
        text.append(u'Author:       {}'.format(','.join(build.getResponsibleUsers())))

        text.append('\nLog from failed builds:')
        summary_step = [x for x in build.getSteps() if x.getName() == 'summary'][0]
        summary_log  = [x for x in summary_step.getLogs() if x.getName() == 'stdio'][0]
        failed_ports = set()
        maintainers_to_notify = set()
        global portWatcherMessageFormatter_pattern
        pattern = portWatcherMessageFormatter_pattern
        # iterate through all the ports being built
        for line in summary_log.getText().splitlines():
            if 'ERROR' not in line:
                continue
            # in case of a build error, print the error and add the broken port(s) to the list
            text.append('\t' + line.replace(' maintainers:', '\n\t> maintainers:'))
            match = pattern.match(line)
            if match:
                port, dependency, maintainers = match.group('port', 'dependency', 'maintainers')
                failed_ports.add(port)
                if dependency:
                    failed_ports.add(dependency)
                if maintainers:
                    maintainers_to_notify.update(maintainers.split(','))

        if failed_ports:
            text.append('\nBroken ports:\n\t- {}'.format('\n\t- '.join(sorted(failed_ports))))
        if maintainers_to_notify:
            text.append('\nResponsible maintainers:\n\t- {}'.format('\n\t- '.join(sorted(maintainers_to_notify))))
            interested_users.update(maintainers_to_notify)

        # links to individual builds
        text.append('\nLinks to individual build jobs:')
        trigger_step = [x for x in build.getSteps() if x.getName() == 'portbuilders'][0]
        # FIXME Sorting is lexicographic and won't work properly for
        # - ports-10.11-x86_64-builder #99
        # - ports-10.11-x86_64-builder #100
        for label, url in sorted(trigger_step.getURLs().iteritems()):
            text.append('- {}\n  {}'.format(label, url))
        text.append('\n-- \nBest regards,\nMacPorts Buildbot\n{}'.format(c['buildbotURL']))

        if failed_ports:
            subject += ': ' + ', '.join(sorted(failed_ports)[:10])
            if len(failed_ports) > 10:
                subject +=  ', and {} more'.format(len(failed_ports) - 10)

    return ({'body': '\n'.join(text), 'type': 'plain', 'subject': subject},
            interested_users)

if config['production']:
    notifyUsers = True
    exception_recipients = ['macports-builds@lists.macports.org', 'admin@macports.org']
    build_recipients = ['macports-builds@lists.macports.org']
else:
    notifyUsers = False
    if 'notifyemail' in config:
        exception_recipients = [config['notifyemail']]
        build_recipients = [config['notifyemail']]

if config['production'] or 'notifyemail' in config:
    c['status'].extend((
        # send mail about base failures to users on the blamelist
        status.MailNotifier(
            fromaddr='buildbot@macports.org',
            extraHeaders={'Reply-To': 'noreply@macports.org'},
            # unless lookup is defined, users have to be configured locally
            # maybe a smarter function is needed, but lookup='' does it for now
            lookup='',
            mode=('problem'),
            builders=base_buildernames,
            extraRecipients=build_recipients,
            #smtpPort=25,
            #relayhost='localhost',
            sendToInterestedUsers=notifyUsers),
        PortsMailNotifier(
            fromaddr='buildbot@macports.org',
            extraHeaders={'Reply-To': 'noreply@macports.org'},
            lookup='',
            mode=('failing'),
            builders=portwatcher_buildernames,
            extraRecipients=build_recipients,
            #smtpPort=25,
            #relayhost='localhost',
            sendToInterestedUsers=notifyUsers,
            portMessageFormatter=portWatcherMessageFormatter),
        # notifications about exceptions
        status.MailNotifier(
            fromaddr='buildbot@macports.org',
            extraHeaders={'Reply-To': 'noreply@macports.org'},
            mode=('exception'),
            extraRecipients=exception_recipients,
            sendToInterestedUsers=notifyUsers)
        ))

####### PROJECT IDENTITY #######

# the 'title' string will appear at the top of this buildbot
# installation's WebStatus home page (linked to the
# 'titleURL') and is embedded in the title of the waterfall HTML page.

c['title'] = config['title']
c['titleURL'] = config['titleurl']
c['buildbotURL'] = config['buildboturl']


####### DATABASE #######

# This specifies what database buildbot uses to store its state. You can
# leave this at its default for all but the largest installations.
c['db'] = {'db_url': config['db_url']}


####### DATA LIFETIME #######
c['buildHorizon'] = config['buildhorizon']
c['logHorizon'] = config['loghorizon']
c['eventHorizon'] = config['eventhorizon']
c['caches'] = {
    'BuildRequests': config['cachedbuildrequests'],
    'Builds': config['cachedbuilds'],
    'Changes': config['cachedchanges'],
    'chdicts': config['cachedchanges'],
    'SourceStamps': config['cachedbuildrequests'],
    'ssdicts': config['cachedbuildrequests']
    }
